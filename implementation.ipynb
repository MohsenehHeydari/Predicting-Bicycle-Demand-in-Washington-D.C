{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import graphviz\n",
    "\n",
    "import sklearn.discriminant_analysis as skl_da\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and visualisation\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('training_data_fall2024.csv')\n",
    "\n",
    "# 2. replacing the missing values by 0 (if there is missing value)\n",
    "df.fillna(method='ffill', inplace=False)\n",
    "\n",
    "# 3. lable encoding: Map 'low_bike_demand' to 0 and 'high_bike_demand' to 1\n",
    "df['increase_stock'] = df['increase_stock'].map({'low_bike_demand': 0, 'high_bike_demand': 1})\n",
    "\n",
    "# 4. feature selection: based on the correlation between the features we choose the required features\n",
    "correlation_matrix = df.corr(method='pearson') \n",
    "# print(correlation_matrix)  \n",
    "\n",
    "# snow might have a constant value = 0\n",
    "is_constant = df['snow'].nunique() == 1  \n",
    "if(is_constant == True):\n",
    "    df = df.drop(columns=['snow']) \n",
    "\n",
    "# it also a good correlation between day_of_week and weekday.\n",
    "df = df.drop(columns=['weekday'])  \n",
    "\n",
    "# it is also a strong correlation between summertime and temp\n",
    "df = df.drop(columns=['summertime']) \n",
    "# it is also a reasonable correlation between temp and dew\n",
    "df = df.drop(columns=['dew']) \n",
    "\n",
    "# 5. Normalize Numerical Features\n",
    "numerical_features = ['temp', 'humidity', 'precip', 'snowdepth', 'windspeed', 'cloudcover', 'visibility']\n",
    "\n",
    "# Initialize the scaler : subtracting the mean and dividing by the standard deviation\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical features and transform\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# 6. Cyclic encoding for cyclic features\n",
    "hour = df['hour_of_day']\n",
    "hour_in_radians = hour * (2 * np.pi / 24)\n",
    "df['hour_sin'] = np.sin(hour_in_radians)\n",
    "df['hour_cos'] = np.cos(hour_in_radians)\n",
    "\n",
    "day_of_week = df['day_of_week']\n",
    "day_in_radians = day_of_week * (2 * np.pi / 7)\n",
    "df['weekday_sin'] = np.sin(day_in_radians)\n",
    "df['weekday_cos'] = np.cos(day_in_radians)\n",
    "\n",
    "month1 = df['month']\n",
    "month_in_radians = month1 * (2 * np.pi / 12)\n",
    "df['month_sin'] = np.sin(month_in_radians)\n",
    "df['month_cos'] = np.cos(month_in_radians)\n",
    "\n",
    "\n",
    "# Drop the original columns\n",
    "df.drop(['hour_of_day', 'day_of_week', 'month'], axis=1, inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df[['increase_stock', 'month_cos']] = df[['month_cos', 'increase_stock']].values\n",
    "\n",
    "# Swap the column names\n",
    "df.columns.values[8], df.columns.values[14] = df.columns.values[14], df.columns.values[8]\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y = df['increase_stock'] # Target variable \n",
    "# X = df.drop('increase_stock', axis=1)  # Features \n",
    "\n",
    "# Split into training and test set\n",
    "X_train = df.iloc[:1280, :14]\n",
    "y_train = df.iloc[:1280, 14:]\n",
    "X_test = df.iloc[1280:, :14]\n",
    "y_test = df.iloc[1280:, 14:]\n",
    "\n",
    "# Convert Y_train and Y_test to 1D arrays \n",
    "y_train = y_train.values.ravel().astype(int)\n",
    "y_test = y_test.values.ravel().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "def implement_lda():\n",
    "    #Initialize LDA with simpler parameters\n",
    "    lda = skl_da.LinearDiscriminantAnalysis(solver='svd')  \n",
    "\n",
    "    param_grid = {\n",
    "        'solver': ['svd', 'lsqr'],  \n",
    "    }\n",
    "\n",
    "    #perform grid search\n",
    "    grid_search = GridSearchCV(lda, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    #get best model\n",
    "    best_lda = grid_search.best_estimator_\n",
    "\n",
    "    # Predictions\n",
    "    predict_prob = best_lda.predict_proba(X_test)\n",
    "    prediction = np.where(predict_prob[:, 0]>=0.5, 0, 1)  # Using numeric labels instead of strings\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nLDA Results:\")\n",
    "    print('Best parameters:', grid_search.best_params_)\n",
    "    print('The class order in the model:', best_lda.classes_)\n",
    "    print('\\nFirst five predicted probabilities:')\n",
    "    with np.printoptions(suppress=True, precision=3):\n",
    "        print(predict_prob[0:5])\n",
    "\n",
    "    print(\"\\nFirst five predictions:\")\n",
    "    print(prediction[0:5])\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(pd.crosstab(prediction, y_test))\n",
    "\n",
    "    accuracy = np.mean(prediction == y_test)\n",
    "    print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
    "\n",
    "    return best_lda, accuracy\n",
    "\n",
    "# 2. Quadratic Discriminant Analysis (QDA)\n",
    "def implement_qda():\n",
    "    #Initialize QDA\n",
    "    qda = skl_da.QuadraticDiscriminantAnalysis()\n",
    "\n",
    "    param_grid = {\n",
    "        'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # Simplified parameter grid\n",
    "    }\n",
    "\n",
    "    #perform grid search\n",
    "    grid_search = GridSearchCV(qda, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    #get best model\n",
    "    best_qda = grid_search.best_estimator_\n",
    "\n",
    "    # Predictions\n",
    "    predict_prob = best_qda.predict_proba(X_test)\n",
    "    prediction = np.where(predict_prob[:, 0]>=0.5, 0, 1)  # Using numeric labels instead of strings\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nQDA Results:\")\n",
    "    print('Best parameters:', grid_search.best_params_)\n",
    "    print('The class order in the model:', best_qda.classes_)\n",
    "    print('\\nFirst five predicted probabilities:')\n",
    "    with np.printoptions(suppress=True, precision=3):\n",
    "        print(predict_prob[0:5])\n",
    "\n",
    "    print(\"\\nFirst five predictions:\")\n",
    "    print(prediction[0:5])\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(pd.crosstab(prediction, y_test))\n",
    "\n",
    "    accuracy = np.mean(prediction == y_test)\n",
    "    print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
    "\n",
    "    return best_qda, accuracy\n",
    "\n",
    "#run both models and compare\n",
    "best_lda, lda_accuracy = implement_lda()\n",
    "best_qda, qda_accuracy = implement_qda()\n",
    "\n",
    "#Compare models\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"LDA Accuracy: {lda_accuracy:.3f}\")\n",
    "print(f\"QDA Accuracy: {qda_accuracy:.3f}\")\n",
    "\n",
    "# Print which model performed better\n",
    "if lda_accuracy > qda_accuracy:\n",
    "    print(\"LDA performed better\")\n",
    "elif qda_accuracy > lda_accuracy:\n",
    "    print(\"QDA performed better\")\n",
    "else:\n",
    "    print(\"Both models performed equally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "#model.fit(X=X_train, y=y_train)\n",
    "#print(model.feature_names_in_)\n",
    "#print(model.classes_)\n",
    "\n",
    "#Grid\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "model = grid_search.best_estimator_\n",
    "print(model)\n",
    "\n",
    "#Tree\n",
    "\n",
    "feature_names = [str(name) for name in X_train.columns]\n",
    "class_names = [str(name) for name in model.classes_]\n",
    "#dot_data = tree.export_graphviz(model, out_file=None, feature_names= X_train.columns, class_names=model.classes_, filled=True, rounded=True, leaves_parallel=True, proportion=True)\n",
    "dot_data = tree.export_graphviz(model, out_file=None, feature_names= X_train.columns, class_names=class_names, filled=True, rounded=True, leaves_parallel=True, proportion=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "\n",
    "\n",
    "#Testing\n",
    "\n",
    "# X_test = test.drop(columns=['increase_stock'])\n",
    "#X_test = pd.get_dummies(X_test, columns = ['Store7'])\n",
    "# y_test = test['increase_stock']\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print('Accuracy rate is %2f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_predict)\n",
    "print('Precision is %2f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, y_predict)\n",
    "print('Recall is %2f' % recall)\n",
    "\n",
    "#f1score = f1_score(y_test, y_predict)\n",
    "#print('f1-Score is %2f' % f1score)\n",
    "\n",
    "pd.crosstab(y_predict, y_test) \n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Per-Class Accuracy\n",
    "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# Display Per-Class Accuracy\n",
    "for idx, accuracy in enumerate(per_class_accuracy):\n",
    "    print(f\"Accuracy for Class {idx}: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision_per_class = conf_matrix.diagonal() / conf_matrix.sum(axis=0)\n",
    "\n",
    "# Display precision for each class\n",
    "for idx, precision in enumerate(precision_per_class):\n",
    "    print(f\"Precision for Class {idx}: {precision:.2f}\")\n",
    "\n",
    "\n",
    "# Calculate Recall for each class\n",
    "recall_per_class = conf_matrix.diagonal() / conf_matrix.sum(axis=0)\n",
    "\n",
    "# Display Recall for each class\n",
    "for idx, recall in enumerate(recall_per_class):\n",
    "    print(f\"Recall for Class {idx}: {recall:.2f}\")\n",
    "\n",
    "\n",
    "# Calculate f1-score for each class\n",
    "f1_per_class = conf_matrix.diagonal() / conf_matrix.sum(axis=0)\n",
    "\n",
    "# Display f1-score for each class\n",
    "for idx, f1_score in enumerate(f1_per_class):\n",
    "    print(f\"f1-score for Class {idx}: {f1_score:.2f}\")\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y_test, y_predict)\n",
    "\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-NN classifier  \n",
    "knn = KNeighborsClassifier()  \n",
    "\n",
    "# Define the grid of k values to search  \n",
    "param_grids = {\n",
    "                'n_neighbors': np.arange(1, 51), \n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric':['euclidean', 'manhattan', 'minkowski'] \n",
    "                }  \n",
    "\n",
    "# Create the grid search object  \n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grids, cv=5, scoring='accuracy', n_jobs=-1)  \n",
    "\n",
    "# Fit the grid search to the training data  \n",
    "grid_search.fit(X_train, y_train)  \n",
    "\n",
    "# Output the best parameters and scores  \n",
    "print(\"Best parameters:\", grid_search.best_params_)  \n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)  \n",
    "\n",
    "# Best model based on grid search  \n",
    "best_knn = grid_search.best_estimator_ \n",
    "\n",
    "# Make predictions on the test set  \n",
    "y_pred = best_knn.predict(X_test)  \n",
    "\n",
    "# Print classification report and confusion matrix for model evaluation  \n",
    "print(\"Classification Report:\")  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "\n",
    "print(\"Confusion Matrix:\")  \n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # Required for 'l1' penalty\n",
    "}\n",
    "\n",
    "# Nested Cross-Validation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "nested_scores = []\n",
    "best_models = []\n",
    "\n",
    "\n",
    "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
    "    # Outer train-validation split\n",
    "    X_outer_train, X_outer_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_outer_train, y_outer_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Inner cross-validation for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=log_reg,\n",
    "        param_grid=param_grid,\n",
    "        cv=inner_cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_outer_train, y_outer_train)\n",
    "    logitic_best_model = grid_search.best_estimator_\n",
    "    best_models.append(logitic_best_model)\n",
    "    \n",
    "    # Evaluate the best model on the validation set of the outer loop\n",
    "    val_predictions = logitic_best_model.predict(X_outer_val)\n",
    "    val_accuracy = accuracy_score(y_outer_val, val_predictions)\n",
    "    nested_scores.append(val_accuracy)\n",
    "\n",
    "# Summary of nested cross-validation\n",
    "print(f\"Nested Cross-Validation Scores (on training set): {nested_scores}\")\n",
    "print(f\"Mean Nested Cross-Validation Score: {np.mean(nested_scores)}\")\n",
    "\n",
    "# Final evaluation on the test set\n",
    "final_model = best_models[np.argmax(nested_scores)]  # Select the best model from nested CV\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "#print(\"\\nFinal Evaluation on Test Set:\")\n",
    "#print(\"Classification Report:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, test_predictions))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Naive Models  \n",
    "class AlwaysHighDemandClassifier:  \n",
    "    def predict(self, X):  \n",
    "        return [1] * len(X)  \n",
    "\n",
    "class AlwaysLowDemandClassifier:  \n",
    "    def predict(self, X):  \n",
    "        return [0] * len(X)  \n",
    "\n",
    "\n",
    "# Evaluate the Naive Models  \n",
    "# Always predict high  \n",
    "naive_high = AlwaysHighDemandClassifier()  \n",
    "naive_high_predictions = naive_high.predict(X_test)  \n",
    "print(\"Always Predict High Results:\")  \n",
    "print(f\"Accuracy: {accuracy_score(y_test, naive_high_predictions)}\")  \n",
    "print(classification_report(y_test, naive_high_predictions))  \n",
    "\n",
    "# Always predict low  \n",
    "naive_low = AlwaysLowDemandClassifier()  \n",
    "naive_low_predictions = naive_low.predict(X_test)  \n",
    "print(\"Always Predict Low Results:\")  \n",
    "print(f\"Accuracy: {accuracy_score(y_test, naive_low_predictions)}\")  \n",
    "print(classification_report(y_test, naive_low_predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "\n",
    "# Load the dataset\n",
    "dff = pd.read_csv('test_data_fall2024.csv')\n",
    "\n",
    "#replacing the missing values by 0 (if there is missing value)\n",
    "dff.fillna(method='ffill', inplace=False)\n",
    "\n",
    "# snow might have a constant value = 0\n",
    "is_constant = dff['snow'].nunique() == 1  \n",
    "if(is_constant == True):\n",
    "    dff = dff.drop(columns=['snow']) \n",
    "#print(dff.info()) \n",
    "\n",
    "# # it also a good correlation between day_of_week and weekday. so, we can use one of them. \n",
    "# # it's better to use day_of_week. because it is more specific\n",
    "dff = dff.drop(columns=['weekday']) \n",
    "\n",
    "# # it is also a strong correlation between summertime and temp\n",
    "# # we can choose the temp because we can expand the final result and conclude if it is summer or not based on the temp.\n",
    "dff = dff.drop(columns=['summertime']) \n",
    "# # it is also a reasonable correlation between temp and dew\n",
    "# # so i eliminated the dew\n",
    "dff = dff.drop(columns=['dew']) \n",
    "\n",
    "# 1. Normalize Numerical Features\n",
    "# List of numerical features to normalize (already identified)\n",
    "numerical_features2 = ['temp', 'humidity', 'precip', 'snowdepth', 'windspeed', 'cloudcover', 'visibility']\n",
    "\n",
    "# Initialize the scaler : subtracting the mean and dividing by the standard deviation\n",
    "# return a transformed dataset which their mean is 0 and the standard deviation is 1\n",
    "scaler = StandardScaler()\n",
    "#print(scaler)\n",
    "# Fit the scaler on the numerical features and transform\n",
    "dff[numerical_features2] = scaler.fit_transform(dff[numerical_features])\n",
    "\n",
    "# 2. Cyclic encoding for cyclic features\n",
    "hour = dff['hour_of_day']\n",
    "hour_in_radians = hour * (2 * np.pi / 24)\n",
    "dff['hour_sin'] = np.sin(hour_in_radians)\n",
    "dff['hour_cos'] = np.cos(hour_in_radians)\n",
    "\n",
    "day_of_week = dff['day_of_week']\n",
    "day_in_radians = day_of_week * (2 * np.pi / 7)\n",
    "dff['weekday_sin'] = np.sin(day_in_radians)\n",
    "dff['weekday_cos'] = np.cos(day_in_radians)\n",
    "\n",
    "month1 = dff['month']\n",
    "month_in_radians = month1 * (2 * np.pi / 12)\n",
    "dff['month_sin'] = np.sin(month_in_radians)\n",
    "dff['month_cos'] = np.cos(month_in_radians)\n",
    "\n",
    "# Drop the original columns\n",
    "dff.drop(['hour_of_day', 'day_of_week', 'month'], axis=1, inplace=True)\n",
    "\n",
    "dff.columns = X_train.columns\n",
    "dff = dff[X_train.columns]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "# df[['increase_stock', 'month_cos']] = df[['month_cos', 'increase_stock']].values\n",
    "\n",
    "# # Swap the column names\n",
    "# df.columns.values[8], df.columns.values[14] = df.columns.values[14], df.columns.values[8]\n",
    "\n",
    "\n",
    "Xx_test = dff.iloc[:, :]\n",
    "\n",
    "\n",
    "predictions = logitic_best_model.predict(Xx_test).astype(int)\n",
    "\n",
    "\n",
    "print(\"\\nFirst five predictions:\")\n",
    "print(predictions[0:5])\n",
    "\n",
    "\n",
    "#verify the number of predictions\n",
    "expected_length = 400\n",
    "actual_length = len(predictions)\n",
    "if actual_length != expected_length:\n",
    "    print(f\"Warning: Expected {expected_length} predictions, but got {actual_length}.\")\n",
    "\n",
    "#convert predictions to a single-row DataFrame\n",
    "predictions_dff = pd.DataFrame([predictions])\n",
    "\n",
    "#save the predictions to 'predictions.csv'\n",
    "predictions_dff.to_csv('predictions.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
